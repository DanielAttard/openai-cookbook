{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to handle rate limits\n",
    "\n",
    "When you call the OpenAI API repeatedly, you may encounter error messages that say `429: 'Too Many Requests'` or `RateLimitError`. These error messages come from exceeding the API's rate limits.\n",
    "\n",
    "Rate limits are a common practice for APIs, and they're put in place for a few different reasons.\n",
    "\n",
    "- First, they help protect against abuse or misuse of the API. For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity.\n",
    "- Second, rate limits help ensure that everyone has fair access to the API. If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that everyone has an opportunity to use the API without experiencing slowdowns.\n",
    "- Lastly, rate limits can help OpenAI manage the aggregate load on its infrastructure. If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users.\n",
    "\n",
    "Although hitting rate limits can be frustrating, rate limits exist to protect the reliable operation of the API for its users.\n",
    "\n",
    "In this guide, we'll share some tips for avoiding and handling rate limit errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default rate limits\n",
    "\n",
    "As of Sep 2022, the default rate limits are:\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>Text Completion &amp; Embedding endpoints</th>\n",
    "    <th>Code &amp; Edit endpoints</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>Free trial users</td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 requests / minute</li>\n",
    "            <li>150,000 tokens / minute</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 requests / minute</li>\n",
    "            <li>150,000 tokens / minute</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Pay-as-you-go users (in your first 48 hours)</td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>60 requests / minute</li>\n",
    "            <li>250,000 davinci tokens / minute (and proportionally more for smaller models)</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 requests / minute</li>\n",
    "            <li>150,000 tokens / minute</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Pay-as-you-go users (after your first 48 hours)</td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>3,000 requests / minute</li>\n",
    "            <li>250,000 davinci tokens / minute (and proportionally more for smaller models)</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 requests / minute</li>\n",
    "            <li>150,000 tokens / minute</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "For reference, 1,000 tokens is roughly a page of text.\n",
    "\n",
    "### Other rate limit resources\n",
    "\n",
    "Read more about OpenAI's rate limits in the [OpenAI Help Center](https://help.openai.com/en/):\n",
    "\n",
    "- [Is API usage subject to any rate limits?](https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits)\n",
    "- [How can I solve 429: 'Too Many Requests' errors?](https://help.openai.com/en/articles/5955604-how-can-i-solve-429-too-many-requests-errors)\n",
    "\n",
    "### Requesting a rate limit increase\n",
    "\n",
    "If you'd like your organization's rate limit increased, please fill out the following form:\n",
    "\n",
    "- [OpenAI Rate Limit Increase Request form](https://forms.gle/56ZrwXXoxAN1yt6i9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example rate limit error\n",
    "\n",
    "A rate limit error will occur when API requests are sent too quickly. If using the OpenAI Python library, they will look something like:\n",
    "\n",
    "```\n",
    "RateLimitError: Rate limit reached for default-codex in organization org-{id} on requests per min. Limit: 20.000000 / min. Current: 24.000000 / min. Contact support@openai.com if you continue to have issues or if you’d like to request an increase.\n",
    "```\n",
    "\n",
    "Below is example code for triggering a rate limit error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# request a bunch of completions in a loop\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcode-cushman-001\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdef magic_function():\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:149\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[1;32m    141\u001b[0m         timeout,\n\u001b[1;32m    142\u001b[0m         stream,\n\u001b[1;32m    143\u001b[0m         headers,\n\u001b[1;32m    144\u001b[0m         request_timeout,\n\u001b[1;32m    145\u001b[0m         typed_api_type,\n\u001b[1;32m    146\u001b[0m         requestor,\n\u001b[1;32m    147\u001b[0m         url,\n\u001b[1;32m    148\u001b[0m         params,\n\u001b[0;32m--> 149\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m__prepare_create_request(\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[1;32m    153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:106\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[0;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    104\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m MAX_TIMEOUT\n\u001b[0;32m--> 106\u001b[0m requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39;49mAPIRequestor(\n\u001b[1;32m    107\u001b[0m     api_key,\n\u001b[1;32m    108\u001b[0m     api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[1;32m    109\u001b[0m     api_type\u001b[39m=\u001b[39;49mapi_type,\n\u001b[1;32m    110\u001b[0m     api_version\u001b[39m=\u001b[39;49mapi_version,\n\u001b[1;32m    111\u001b[0m     organization\u001b[39m=\u001b[39;49morganization,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    113\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[1;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    115\u001b[0m     deployment_id,\n\u001b[1;32m    116\u001b[0m     engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m     params,\n\u001b[1;32m    125\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:130\u001b[0m, in \u001b[0;36mAPIRequestor.__init__\u001b[0;34m(self, key, api_base, api_type, api_version, organization)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    123\u001b[0m     key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     organization\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    128\u001b[0m ):\n\u001b[1;32m    129\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_base \u001b[39m=\u001b[39m api_base \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_base\n\u001b[0;32m--> 130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m key \u001b[39mor\u001b[39;00m util\u001b[39m.\u001b[39;49mdefault_api_key()\n\u001b[1;32m    131\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_type \u001b[39m=\u001b[39m (\n\u001b[1;32m    132\u001b[0m         ApiType\u001b[39m.\u001b[39mfrom_str(api_type)\n\u001b[1;32m    133\u001b[0m         \u001b[39mif\u001b[39;00m api_type\n\u001b[1;32m    134\u001b[0m         \u001b[39melse\u001b[39;00m ApiType\u001b[39m.\u001b[39mfrom_str(openai\u001b[39m.\u001b[39mapi_type)\n\u001b[1;32m    135\u001b[0m     )\n\u001b[1;32m    136\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_version \u001b[39m=\u001b[39m api_version \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_version\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/util.py:186\u001b[0m, in \u001b[0;36mdefault_api_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[39mreturn\u001b[39;00m openai\u001b[39m.\u001b[39mapi_key\n\u001b[1;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mAuthenticationError(\n\u001b[1;32m    187\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo API key provided. You can set your API key in code using \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key = <API-KEY>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key_path = <PATH>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m     )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions."
     ]
    }
   ],
   "source": [
    "import openai  # for making OpenAI API requests\n",
    "\n",
    "# request a bunch of completions in a loop\n",
    "for _ in range(100):\n",
    "    openai.Completion.create(\n",
    "        model=\"code-cushman-001\",\n",
    "        prompt=\"def magic_function():\\n\\t\",\n",
    "        max_tokens=10,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to avoid rate limit errors\n",
    "\n",
    "### Retrying with exponential backoff\n",
    "\n",
    "One easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached.\n",
    "\n",
    "This approach has many benefits:\n",
    "\n",
    "- Automatic retries means you can recover from rate limit errors without crashes or missing data\n",
    "- Exponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail\n",
    "- Adding random jitter to the delay helps retries from all hitting at the same time\n",
    "\n",
    "Note that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won’t work.\n",
    "\n",
    "Below are a few example solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example #1: Using the Tenacity library\n",
    "\n",
    "[Tenacity](https://tenacity.readthedocs.io/en/latest/) is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything.\n",
    "\n",
    "To add exponential backoff to your requests, you can use the `tenacity.retry` [decorator](https://peps.python.org/pep-0318/). The following example uses the `tenacity.wait_random_exponential` function to add random exponential backoff to a request.\n",
    "\n",
    "Note that the Tenacity library is a third-party tool, and OpenAI makes no guarantees about its reliability or security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-5oowO391reUW8RGVfFyzBM1uBs4A5 at 0x10d8cae00> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" a little girl dreamed of becoming a model.\\n\\nNowadays, that dream\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1662793900,\n",
       "  \"id\": \"cmpl-5oowO391reUW8RGVfFyzBM1uBs4A5\",\n",
       "  \"model\": \"text-davinci-002\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai  # for OpenAI API calls\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "\n",
    "completion_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example #2: Using the backoff library\n",
    "\n",
    "Another library that provides function decorators for backoff and retry is [backoff](https://pypi.org/project/backoff/).\n",
    "\n",
    "Like Tenacity, the backoff library is a third-party tool, and OpenAI makes no guarantees about its reliability or security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-5oowPhIdUvshEsF1rBhhwE9KFfI3M at 0x111043680> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" two children lived in a poor country village. In the winter, the temperature would\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1662793901,\n",
       "  \"id\": \"cmpl-5oowPhIdUvshEsF1rBhhwE9KFfI3M\",\n",
       "  \"model\": \"text-davinci-002\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import backoff  # for exponential backoff\n",
    "import openai  # for OpenAI API calls\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def completions_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "\n",
    "completions_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: Manual backoff implementation\n",
    "\n",
    "If you don't want to use third-party libraries, you can implement your own backoff logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-5oowRsCXv3AkUgVJyyo3TQrVq7hIT at 0x111024220> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" a man decided to greatly improve his karma by turning his life around.\\n\\n\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1662793903,\n",
       "  \"id\": \"cmpl-5oowRsCXv3AkUgVJyyo3TQrVq7hIT\",\n",
       "  \"model\": \"text-davinci-002\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import random\n",
    "import time\n",
    "\n",
    "import openai\n",
    "\n",
    "# define a retry decorator\n",
    "def retry_with_exponential_backoff(\n",
    "    func,\n",
    "    initial_delay: float = 1,\n",
    "    exponential_base: float = 2,\n",
    "    jitter: bool = True,\n",
    "    max_retries: int = 10,\n",
    "    errors: tuple = (openai.error.RateLimitError,),\n",
    "):\n",
    "    \"\"\"Retry a function with exponential backoff.\"\"\"\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Initialize variables\n",
    "        num_retries = 0\n",
    "        delay = initial_delay\n",
    "\n",
    "        # Loop until a successful response or max_retries is hit or an exception is raised\n",
    "        while True:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "\n",
    "            # Retry on specified errors\n",
    "            except errors as e:\n",
    "                # Increment retries\n",
    "                num_retries += 1\n",
    "\n",
    "                # Check if max retries has been reached\n",
    "                if num_retries > max_retries:\n",
    "                    raise Exception(\n",
    "                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
    "                    )\n",
    "\n",
    "                # Increment the delay\n",
    "                delay *= exponential_base * (1 + jitter * random.random())\n",
    "\n",
    "                # Sleep for the delay\n",
    "                time.sleep(delay)\n",
    "\n",
    "            # Raise exceptions for any errors not specified\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@retry_with_exponential_backoff\n",
    "def completions_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "\n",
    "completions_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to maximize throughput of batch processing given rate limits\n",
    "\n",
    "If you're processing real-time requests from users, backoff and retry is a great strategy to minimize latency while avoiding rate limit errors.\n",
    "\n",
    "However, if you're processing large volumes of batch data, where throughput matters more than latency, there are a few other things you can do in addition to backoff and retry.\n",
    "\n",
    "### Proactively adding delay between requests\n",
    "\n",
    "If you are constantly hitting the rate limit, then backing off, then hitting the rate limit again, then backing off again, it's possible that a good fraction of your request budget will be 'wasted' on requests that need to be retried. This limits your processing throughput, given a fixed rate limit.\n",
    "\n",
    "Here, one potential solution is to calculate your rate limit and add a delay equal to its reciprocal (e.g., if your rate limit 20 requests per minute, add a delay of 3 seconds to each request). This can help you operate near the rate limit ceiling without hitting it and incurring wasted requests.\n",
    "\n",
    "#### Example of adding delay to a request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-5oowVVZnAzdCPtUJ0rifeamtLcZRp at 0x11b2c7680> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" there was an idyllic little farm that sat by a babbling brook\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1662793907,\n",
       "  \"id\": \"cmpl-5oowVVZnAzdCPtUJ0rifeamtLcZRp\",\n",
       "  \"model\": \"text-davinci-002\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import time\n",
    "import openai\n",
    "\n",
    "# Define a function that adds a delay to a Completion API call\n",
    "def delayed_completion(delay_in_seconds: float = 1, **kwargs):\n",
    "    \"\"\"Delay a completion by a specified amount of time.\"\"\"\n",
    "\n",
    "    # Sleep for the delay\n",
    "    time.sleep(delay_in_seconds)\n",
    "\n",
    "    # Call the Completion API and return the result\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "\n",
    "# Calculate the delay based on your rate limit\n",
    "rate_limit_per_minute = 20\n",
    "delay = 60.0 / rate_limit_per_minute\n",
    "\n",
    "delayed_completion(\n",
    "    delay_in_seconds=delay,\n",
    "    model=\"text-davinci-002\",\n",
    "    prompt=\"Once upon a time,\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Batching requests\n",
    "\n",
    "The OpenAI API has separate limits for requests per minute and tokens per minute.\n",
    "\n",
    "If you're hitting the limit on requests per minute, but have headroom on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with the smaller models.\n",
    "\n",
    "Sending in a batch of prompts works exactly the same as a normal API call, except that pass in a list of strings to `prompt` parameter instead of a single string.\n",
    "\n",
    "**Warning:** the response object may not return completions in the order of the prompts, so always remember to match responses back to prompts using the `index` field.\n",
    "\n",
    "#### Example without batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, before there were grandiloquent tales of the massacre at Fort Mims, there were stories of\n",
      "Once upon a time, a full-sized search and rescue was created. However, CIDIs are the addition of requiring\n",
      "Once upon a time, Schubert was hot with the films. “Schubert sings of honey, flowers,\n",
      "Once upon a time, you could watch these films on your VCR, sometimes years after their initial theatrical release, and there\n",
      "Once upon a time, there was a forest. In that forest, the forest animals ruled. The forest animals had their homes\n",
      "Once upon a time, there were two programs that complained about false positive scans. Peacock and Midnight Manager alike, only\n",
      "Once upon a time, a long, long time ago, tragedy struck. it was the darkest of nights, and there was\n",
      "Once upon a time, when Adam was a perfect little gentleman, he was presented at Court as a guarantee of good character.\n",
      "Once upon a time, Adam and Eve made a mistake. They ate the fruit from the tree of immortality and split the consequences\n",
      "Once upon a time, there was a set of programming fundamental principles known as the \"X model.\" This is a set of\n"
     ]
    }
   ],
   "source": [
    "import openai  # for making OpenAI API requests\n",
    "\n",
    "\n",
    "num_stories = 10\n",
    "prompt = \"Once upon a time,\"\n",
    "\n",
    "# serial example, with one story completion per request\n",
    "for _ in range(num_stories):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"curie\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=20,\n",
    "    )\n",
    "\n",
    "    # print story\n",
    "    print(prompt + response.choices[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example with batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there were two sisters, Eliza Pickering and Ariana 'Ari' Lucas. When these lovely\n",
      "Once upon a time, Keene was stung by a worm — actually, probably a python — snaking through his leg\n",
      "Once upon a time, there was a professor of physics during the depression. It was difficult, during this time, to get\n",
      "Once upon a time, before you got sick, you told stories to all and sundry, and your listeners believed in you\n",
      "Once upon a time, there was one very old nice donkey. He was incredibly smart, in a very old, kind of\n",
      "Once upon a time, the property of a common lodging house was a common cup for all the inhabitants. Betimes a constant\n",
      "Once upon a time, in an unspecified country, there was a witch who had an illegal product. It was highly effective,\n",
      "Once upon a time, a long time ago, I turned 13, my beautiful dog Duncan swept me up into his jaws like\n",
      "Once upon a time, as a thoroughly reformed creature from an army of Nazis, he took On Judgement Day myself and his\n",
      "Once upon a time, Capcom made a game for the Atari VCS called Missile Command. While it was innovative at the time\n"
     ]
    }
   ],
   "source": [
    "import openai  # for making OpenAI API requests\n",
    "\n",
    "\n",
    "num_stories = 10\n",
    "prompts = [\"Once upon a time,\"] * num_stories\n",
    "\n",
    "# batched example, with 10 stories completions per request\n",
    "response = openai.Completion.create(\n",
    "    model=\"curie\",\n",
    "    prompt=prompts,\n",
    "    max_tokens=20,\n",
    ")\n",
    "\n",
    "# match completions to prompts by index\n",
    "stories = [\"\"] * len(prompts)\n",
    "for choice in response.choices:\n",
    "    stories[choice.index] = prompts[choice.index] + choice.text\n",
    "\n",
    "# print stories\n",
    "for story in stories:\n",
    "    print(story)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
